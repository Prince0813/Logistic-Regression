{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theroy Questiona\n"
      ],
      "metadata": {
        "id": "JY6gNd-EnU9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What is Logistic Regression, and how does it differ from Linear Regression?**  \n",
        "   - Logistic Regression is a classification algorithm used for binary or multiclass classification, whereas Linear Regression is used for predicting continuous values. Logistic Regression applies a **sigmoid function** to map outputs between 0 and 1, making it suitable for probability-based classification.\n",
        "\n",
        "2. **What is the mathematical equation of Logistic Regression?**  \n",
        "   - \\( P(Y=1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n)}} \\)\n",
        "\n",
        "3. **Why do we use the Sigmoid function in Logistic Regression?**  \n",
        "   - The Sigmoid function maps any real-valued number to a probability range between 0 and 1, making it suitable for classification tasks.\n",
        "\n",
        "4. **What is the cost function of Logistic Regression?**  \n",
        "   - Logistic Regression uses **Log Loss (Binary Cross-Entropy Loss)**:  \n",
        "     \\[\n",
        "     J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right]\n",
        "     \\]\n",
        "     where \\( h_\\theta(x_i) \\) is the sigmoid function.\n",
        "\n",
        "5. **What is Regularization in Logistic Regression? Why is it needed?**  \n",
        "   - Regularization prevents **overfitting** by penalizing large coefficients. It helps in generalizing the model better.\n",
        "\n",
        "6. **Explain the difference between Lasso, Ridge, and Elastic Net regression.**  \n",
        "   - **Lasso Regression (L1 penalty)**: Shrinks some coefficients to zero, performing feature selection.  \n",
        "   - **Ridge Regression (L2 penalty)**: Shrinks coefficients but doesn’t set them to zero.  \n",
        "   - **Elastic Net**: Combination of L1 and L2 penalties, balancing feature selection and shrinkage.\n",
        "\n",
        "7. **When should we use Elastic Net instead of Lasso or Ridge?**  \n",
        "   - When dealing with **highly correlated features**, Elastic Net is preferred as it balances feature selection (Lasso) and shrinkage (Ridge).\n",
        "\n",
        "8. **What is the impact of the regularization parameter (λ) in Logistic Regression?**  \n",
        "   - **Higher λ** increases regularization, reducing overfitting but possibly underfitting.  \n",
        "   - **Lower λ** reduces regularization, potentially leading to overfitting.\n",
        "\n",
        "9. **What are the key assumptions of Logistic Regression?**  \n",
        "   - No **multicollinearity** among independent variables.  \n",
        "   - Log-odds of the dependent variable is **linearly related** to independent variables.  \n",
        "   - Large sample size is preferred.\n",
        "\n",
        "10. **What are some alternatives to Logistic Regression for classification tasks?**  \n",
        "    - Decision Trees, Random Forest, SVM, Naïve Bayes, k-NN, Neural Networks.\n",
        "\n",
        "11. **What are Classification Evaluation Metrics?**  \n",
        "    - Accuracy, Precision, Recall, F1-score, ROC-AUC, Log Loss.\n",
        "\n",
        "12. **How does class imbalance affect Logistic Regression?**  \n",
        "    - It biases the model towards the majority class. Using **class weights** or **resampling techniques** can address this issue.\n",
        "\n",
        "13. **What is Hyperparameter Tuning in Logistic Regression?**  \n",
        "    - It involves optimizing parameters like **regularization strength (λ)** and **solver type** using methods like **Grid Search or Random Search**.\n",
        "\n",
        "14. **What are different solvers in Logistic Regression? Which one should be used?**  \n",
        "    - **liblinear** (for small datasets), **saga** (for large datasets with L1/L2 regularization), **lbfgs** (default, for multiclass problems), **newton-cg** (for smooth optimization).\n",
        "\n",
        "15. **How is Logistic Regression extended for multiclass classification?**  \n",
        "    - Using **One-vs-Rest (OvR)** or **Softmax Regression (Multinomial Logistic Regression).**\n",
        "\n",
        "16. **What are the advantages and disadvantages of Logistic Regression?**  \n",
        "    - **Advantages**: Simple, interpretable, efficient for small datasets.  \n",
        "    - **Disadvantages**: Assumes linear decision boundary, sensitive to multicollinearity.\n",
        "\n",
        "17. **What are some use cases of Logistic Regression?**  \n",
        "    - Spam detection, fraud detection, medical diagnosis, credit risk analysis.\n",
        "\n",
        "18. **What is the difference between Softmax Regression and Logistic Regression?**  \n",
        "    - **Logistic Regression** is for **binary classification**, whereas **Softmax Regression** is for **multiclass classification**, assigning probabilities to each class.\n",
        "\n",
        "19. **How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**  \n",
        "    - **OvR** is useful when the classes are **imbalanced** or when computational efficiency is needed.  \n",
        "    - **Softmax** is better when you need **mutually exclusive** class probabilities.\n",
        "\n",
        "20. **How do we interpret coefficients in Logistic Regression?**  \n",
        "    - The coefficients represent the **log-odds** change for a one-unit increase in the predictor variable, holding other variables constant.  \n",
        "    - \\( e^{\\beta_i} \\) gives the **odds ratio**."
      ],
      "metadata": {
        "id": "CRw0HhFsnJg3"
      }
    }
  ]
}